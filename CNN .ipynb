{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807697ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e796aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load CIFAR-10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de06e616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=5,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e86c825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the CNN Architecture\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Convolutional layer 1 with 3 input channels (for RGB images), 6 output channels, and 5x5 kernel\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # Max pooling layer with a 2x2 window\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Convolutional layer 2 with 6 input channels (from the previous layer), 16 output channels, and 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 20)\n",
    "        self.fc3 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ceb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Your CNN\n",
    "\n",
    "dataset  = trainset\n",
    "\n",
    "# Initialize your CNN model\n",
    "cnn = Net()\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = torch.optim.SGD(cnn.parameters(), lr=0.001, momentum=0.9)  # Stochastic Gradient Descent optimizer\n",
    "# Split your data into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=4, shuffle=False)\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients to avoid accumulation\n",
    "        outputs = cnn(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update the model parameters\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Model\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# Set the model to evaluation mode\n",
    "cnn.eval()\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        images, labels = data\n",
    "        outputs = cnn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Accuracy on the validation set: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(cnn.state_dict(), 'trained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load('trained_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd300fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def cargar_modelo_y_etiquetas(ruta_modelo):\n",
    "    # Cargar el checkpoint (modelo y etiquetas)\n",
    "    checkpoint = torch.load(ruta_modelo)\n",
    "    modelo = ('trained_model.pth')  # Reemplaza 'MiModelo' con tu clase de modelo\n",
    "    modelo.load_state_dict(torch.load['modelo_state_dict'])\n",
    "    etiquetas = checkpoint['class']\n",
    "    return modelo, etiquetas\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822bd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing New Images\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load and preprocess a new image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize the image to match the model's input size\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "image_path = 'image1.jpg'\n",
    "image = Image.open(image_path)\n",
    "input_tensor = transform(image)\n",
    "input_tensor = input_tensor.unsqueeze(0)  # Add a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e86eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on the preprocessed image\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    _, predicted_class = output.max(1)\n",
    "\n",
    "# Print the predicted class\n",
    "print(f'Predicted class: {predicted_class.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e84e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = Image.open(image_path)\n",
    "    input_tensor = transform(image)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        _, predicted_class = output.max(1)\n",
    "    return predicted_class.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'image1.jpg'\n",
    "predict_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b88a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path2 = 'image2.jpg'\n",
    "predict_image(image_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdca486",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path3 = 'image3.jpg'\n",
    "predict_image(image_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf60cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path4 = 'image4.jpeg'\n",
    "predict_image(image_path4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path ='imagen5.jpg'\n",
    "predict_image(image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn virtual env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
